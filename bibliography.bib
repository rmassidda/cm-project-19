@book{trefethen_numerical_1997,
	address = {Philadelphia},
	title = {Numerical linear algebra},
	isbn = {978-0-89871-361-9},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Trefethen, Lloyd N. and Bau, David},
	year = {1997},
	keywords = {Algebras, Linear, Numerical calculations}
}

@book{oliphant_guide_2015,
	address = {Austin, Tex.},
	title = {Guide to {NumPy}},
	isbn = {978-1-5173-0007-4},
	language = {English},
	publisher = {Continuum Press},
	author = {Oliphant, Travis E},
	year = {2015},
	note = {OCLC: 982090469}
}

@book{nocedal_numerical_2006,
	address = {New York},
	edition = {2nd ed},
	series = {Springer series in operations research},
	title = {Numerical optimization},
	isbn = {978-0-387-30303-1},
	language = {en},
	publisher = {Springer},
	author = {Nocedal, Jorge and Wright, Stephen J.},
	year = {2006},
	note = {OCLC: ocm68629100},
	keywords = {Mathematical optimization},
	file = {Numerical Optimization.pdf:/home/rmassidda/onedrive/books/Numerical Optimization.pdf:application/pdf}
}

@article{dauphin_identifying_2014,
	title = {Identifying and attacking the saddle point problem in high-dimensional non-convex optimization},
	url = {http://arxiv.org/abs/1406.2572},
	abstract = {A central challenge to many ﬁelds of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difﬁculty for these local methods to ﬁnd the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difﬁculty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance. This work extends the results of Pascanu et al. (2014).},
	language = {en},
	urldate = {2020-01-05},
	journal = {arXiv:1406.2572 [cs, math, stat]},
	author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2572},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	annote = {Comment: The theoretical review and analysis in this article draw heavily from arXiv:1405.4604 [cs.LG]},
	file = {arxiv.pdf:/home/rmassidda/cm-2019/project/material/arxiv.pdf:application/pdf}
}

@article{vinyals_krylov_nodate,
	title = {Krylov {Subspace} {Descent} for {Deep} {Learning}},
	abstract = {In this paper, we propose a second order optimization method to learn models where both the dimensionality of the parameter space and the number of training samples is high. In our method, we construct on each iteration a Krylov subspace formed by the gradient and an approximation to the Hessian matrix, and then use a subset of the training data samples to optimize over this subspace. As with the Hessian Free (HF) method of Martens (2010), the Hessian matrix is never explicitly constructed, and is computed using a subset of data. In practice, as in HF, we typically use a positive definite substitute for the Hessian matrix such as the Gauss-Newton matrix. We investigate the eﬀectiveness of our proposed method on deep neural networks, and compare its performance to widely used methods such as stochastic gradient descent, conjugate gradient descent and L-BFGS, and also to HF. Our method leads to faster convergence than either L-BFGS or HF, and generally performs better than either of them in cross-validation accuracy. It is also simpler and more general than HF, as it does not require a positive semideﬁnite approximation of the Hessian matrix to work well nor the setting of a damping parameter. The chief drawback versus HF is the need for memory to store a basis for the Krylov subspace.},
	language = {en},
	author = {Vinyals, Oriol and Povey, Daniel},
	pages = {8},
	file = {Vinyals and Povey - Krylov Subspace Descent for Deep Learning.pdf:/home/rmassidda/Zotero/storage/L4S43V5C/Vinyals and Povey - Krylov Subspace Descent for Deep Learning.pdf:application/pdf}
}

@article{robitaille_modified_1996,
	title = {Modified quasi-{Newton} methods for training neural networks},
	volume = {20},
	issn = {0098-1354},
	url = {http://www.sciencedirect.com/science/article/pii/0098135495002286},
	doi = {10.1016/0098-1354(95)00228-6},
	abstract = {The backpropagation algorithm is the most popular procedure to train self-learning feed-forward neural networks. However, the convergence of this algorithm is slow, it being mainly a steepest descent method. Several researchers have proposed other approaches to improve the convergence: conjugate gradient methods, dynamic modification of learning parameters, quasi-Newton or Newton methods, stochastic methods, etc. Quasi-Newton methods were criticized because they require significant computation time and memory space to perform the update of the Hessian matrix limiting their use to middle-sized problems. This paper proposes three variations of the classical approach of the quasi-Newton method that take into account the structure of the network. By neglecting some second-order interactions, the sizes of the resulting approximated Hessian matrices are not proportional to the square of the total number of weights in the network but depend on the number of neurons of each level. The modified quasi-Newton methods are tested on two examples and are compared to classical approaches like regular quasi-Newton methods, backpropagation and conjugate gradient methods. The numerical results show that one of these approaches, named BFGS-N, represents a clear gain in terms of computational time, on large-scale problems, over the traditional methods without the requirement of large memory space.},
	language = {en},
	number = {9},
	urldate = {2020-07-30},
	journal = {Computers \& Chemical Engineering},
	author = {Robitaille, B. and Marcos, B. and Veillette, M. and Payre, G.},
	month = sep,
	year = {1996},
	pages = {1133--1140},
	file = {ScienceDirect Snapshot:/home/rmassidda/Zotero/storage/BCSFBVZI/0098135495002286.html:text/html;ScienceDirect Full Text PDF:/home/rmassidda/Zotero/storage/7I4EGITK/Robitaille et al. - 1996 - Modified quasi-Newton methods for training neural .pdf:application/pdf}
}

@article{pearlmutter_fast_1994,
	title = {Fast {Exact} {Multiplication} by the {Hessian}},
	volume = {6},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1994.6.1.147},
	doi = {10.1162/neco.1994.6.1.147},
	abstract = {Just storing the Hessian H (the matrix of second derivatives δ2E/δwiδwj of the error E with respect to each pair of weights) of a large neural network is difficult. Since a common use of a large matrix like H is to compute its product with various vectors, we derive a technique that directly calculates Hv, where v is an arbitrary vector. To calculate Hv, we first define a differential operator Rv\{f(w)\} = (δ/δr)f(w + rv){\textbar}r=0, note that Rv\{▽w\} = Hv and Rv\{w\} = v, and then apply Rv\{·\} to the equations used to compute ▽w. The result is an exact and numerically stable procedure for computing Hv, which takes about as much computation, and is about as local, as a gradient evaluation. We then apply the technique to a one pass gradient calculation algorithm (backpropagation), a relaxation gradient calculation algorithm (recurrent backpropagation), and two stochastic gradient calculation algorithms (Boltzmann machines and weight perturbation). Finally, we show that this technique can be used at the heart of many iterative techniques for computing various properties of H, obviating any need to calculate the full Hessian.},
	number = {1},
	urldate = {2020-07-30},
	journal = {Neural Computation},
	author = {Pearlmutter, Barak A.},
	month = jan,
	year = {1994},
	note = {Publisher: MIT Press},
	pages = {147--160},
	file = {Snapshot:/home/rmassidda/Zotero/storage/JT8Z4RDM/neco.1994.6.1.html:text/html;Full Text PDF:/home/rmassidda/Zotero/storage/UE253HKX/Pearlmutter - 1994 - Fast Exact Multiplication by the Hessian.pdf:application/pdf}
}

@article{al-baali_efficient_1986,
	title = {An efficient line search for nonlinear least squares},
	volume = {48},
	issn = {0022-3239, 1573-2878},
	url = {http://link.springer.com/10.1007/BF00940566},
	doi = {10.1007/BF00940566},
	abstract = {The line search subproblem in unconstrained optimization is concerned with finding an acceptable steplength which satisfies certain standard conditions. Prototype algorithms are described which guaran{\textasciitilde} tee finding such a step in a finite number of operations. This is achieved by first bracketing an interval of acceptable values and then reducing this bracket uniformly by the repeated use of sectioning in a systematic way. Some new theorems about convergence and termination of the line search are presented.},
	language = {en},
	number = {3},
	urldate = {2020-09-26},
	journal = {Journal of Optimization Theory and Applications},
	author = {Al-Baali, M. and Fletcher, R.},
	month = mar,
	year = {1986},
	pages = {359--377},
	file = {Al-Baali and Fletcher - 1986 - An efficient line search for nonlinear least squar.pdf:/home/rmassidda/Zotero/storage/XYKQMNLC/Al-Baali and Fletcher - 1986 - An efficient line search for nonlinear least squar.pdf:application/pdf}
}

@article{byrd_tool_1989,
	title = {A {Tool} for the {Analysis} of {Quasi}-{Newton} {Methods} with {Application} to {Unconstrained} {Minimization}},
	volume = {26},
	issn = {0036-1429, 1095-7170},
	url = {http://epubs.siam.org/doi/10.1137/0726042},
	doi = {10.1137/0726042},
	abstract = {The BFGS update formula is shown to have an important property that is independent of the algorithmic context of the update, and that is relevant to both constrained and unconstrained optimization. The BFGS method for unconstrained optimization, using a variety of line searches, including backtracking, is shown to be globally and superlinearly convergent on uniformly convex problems. The analysis is particularly simple due to the use of some new tools introduced in this paper.},
	language = {en},
	number = {3},
	urldate = {2020-09-26},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Byrd, Richard H. and Nocedal, Jorge},
	month = jun,
	year = {1989},
	pages = {727--739},
	file = {Byrd and Nocedal - 1989 - A Tool for the Analysis of Quasi-Newton Methods wi.pdf:/home/rmassidda/Zotero/storage/YQ65D48L/Byrd and Nocedal - 1989 - A Tool for the Analysis of Quasi-Newton Methods wi.pdf:application/pdf}
}

@article{fletcher_practical_nodate,
	title = {Practical {Methods} of {Optimization}},
	language = {en},
	author = {Fletcher, Robert},
	pages = {448},
	file = {Fletcher - Practical Methods of Optimization.pdf:/home/rmassidda/Zotero/storage/RXPYFXYK/Fletcher - Practical Methods of Optimization.pdf:application/pdf}
}

@article{kim_tackling_2010,
	title = {Tackling {Box}-{Constrained} {Optimization} via a {New} {Projected} {Quasi}-{Newton} {Approach}},
	volume = {32},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/08073812X},
	doi = {10.1137/08073812X},
	abstract = {Numerous scientiﬁc applications across a variety of ﬁelds depend on box-constrained convex optimization. Box-constrained problems therefore continue to attract research interest. We address box-constrained (strictly convex) problems by deriving two new quasi-Newton algorithms. Our algorithms are positioned between the projected-gradient [J. B. Rosen, J. SIAM, 8(1), 1960, pp. 181–217], and projected-Newton [D. P. Bertsekas, SIAM J. Cont. \& Opt., 20(2), 1982, pp. 221–246] methods. We also prove their convergence under a simple Armijo step-size rule. We provide experimental results for two particular box-constrained problems: nonnegative least squares (NNLS), and nonnegative Kullback-Leibler (NNKL) minimization. For both NNLS and NNKL our algorithms perform competitively against well-established methods on medium-sized problems; for larger problems our approach frequently outperforms the competition.},
	language = {en},
	number = {6},
	urldate = {2020-09-26},
	journal = {SIAM Journal on Scientific Computing},
	author = {Kim, Dongmin and Sra, Suvrit and Dhillon, Inderjit S.},
	month = jan,
	year = {2010},
	pages = {3548--3563},
	file = {Kim et al. - 2010 - Tackling Box-Constrained Optimization via a New Pr.pdf:/home/rmassidda/Zotero/storage/AP69W2L9/Kim et al. - 2010 - Tackling Box-Constrained Optimization via a New Pr.pdf:application/pdf}
}

@article{liu_limited_1989,
	title = {On the limited memory {BFGS} method for large scale optimization},
	volume = {45},
	issn = {0025-5610, 1436-4646},
	url = {http://link.springer.com/10.1007/BF01589116},
	doi = {10.1007/BF01589116},
	abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckleyand LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence On uniformly convex problems.},
	language = {en},
	number = {1-3},
	urldate = {2020-09-26},
	journal = {Mathematical Programming},
	author = {Liu, Dong C. and Nocedal, Jorge},
	month = aug,
	year = {1989},
	pages = {503--528},
	file = {Liu and Nocedal - 1989 - On the limited memory BFGS method for large scale .pdf:/home/rmassidda/Zotero/storage/GRZLERHS/Liu and Nocedal - 1989 - On the limited memory BFGS method for large scale .pdf:application/pdf}
}

@book{di_pillo_high_2003,
	address = {Boston, MA},
	series = {Applied {Optimization}},
	title = {High {Performance} {Algorithms} and {Software} for {Nonlinear} {Optimization}},
	volume = {82},
	isbn = {978-1-4613-7956-0 978-1-4613-0241-4},
	url = {http://link.springer.com/10.1007/978-1-4613-0241-4},
	language = {en},
	urldate = {2020-09-26},
	publisher = {Springer US},
	editor = {Di Pillo, Gianni and Murli, Almerico and Pardalos, Panos M. and Hearn, Donald W.},
	year = {2003},
	doi = {10.1007/978-1-4613-0241-4},
	file = {Di Pillo and Murli - 2003 - High Performance Algorithms and Software for Nonli.pdf:/home/rmassidda/Zotero/storage/T5Z29G2I/Di Pillo and Murli - 2003 - High Performance Algorithms and Software for Nonli.pdf:application/pdf}
}

@book{department_of_mathematics_optimization_linkoping_university_large-scale_2014,
	title = {Large-{Scale} {Optimization} {Methods} with {Application} to {Design} of {Filter} {Networks}},
	isbn = {978-91-7519-456-1},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-103646},
	language = {en},
	urldate = {2020-09-26},
	publisher = {Linköping University Electronic Press},
	author = {{Department of Mathematics, Optimization, Linköping University} and Zikrin, Spartak},
	month = feb,
	year = {2014},
	doi = {10.3384/diss.diva-103646},
	file = {Department of Mathematics, Optimization, Linköping University and Zikrin - 2014 - Large-Scale Optimization Methods with Application .pdf:/home/rmassidda/Zotero/storage/NT58RI2G/Department of Mathematics, Optimization, Linköping University and Zikrin - 2014 - Large-Scale Optimization Methods with Application .pdf:application/pdf}
}

@article{byrd_limited_1995,
	title = {A limited memory algorithm for bound constrained optimization},
	volume = {16},
	number = {5},
	journal = {SIAM Journal on scientific computing},
	author = {Byrd, Richard H. and Lu, Peihuang and Nocedal, Jorge and Zhu, Ciyou},
	year = {1995},
	note = {ISBN: 1064-8275
Publisher: SIAM},
	pages = {1190--1208},
	file = {Byrd et al. - A Limited Memory Algorithm for Bound Constrained Optimization.pdf:/home/rmassidda/Zotero/storage/RVW9LTRJ/Byrd et al. - A Limited Memory Algorithm for Bound Constrained Optimization.pdf:application/pdf}
}
